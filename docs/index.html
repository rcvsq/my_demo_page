<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering">
  <meta name="keywords" content="Model Distillation, Automatic Datataset Creation, Court Homography, Player Detection, Tennis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SVRaster</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/script.js"></script>

</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ðŸŽ¾ Tennis Autodistillation: Dataset Generation and Model Training</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.github.com/rafaelpadilla">Rafael Padilla</a></span>
         </div>

<!--
          affiliations
          <div class="subtitle is-size-5">
            <sup>1</sup> Nvidia &nbsp;
            <sup>2</sup> Cornell University &nbsp;
            <sup>3</sup> National Taiwan University
          </div>
-->
          <!-- TODO: update paper and video link once upload -->
          <div class="column has-text-centered">
            <!-- <div class="publication-links"> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rafaelpadilla/tennis_autodistill"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
             <span class="link-block">
              <a href="./tennis_autodistillation_slides.pdf"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Slides (2.9M)</span>
              </a>
              </span>
           </div>
        </div>

      </div>
    </div>
    </div>
    </div>
    </div>
  </section>


  <!-- video demo -->
  <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="video-container">
        <video autoplay controls muted loop playsinline height="75%">
          <source src="./videos/tennis_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <p>Automated player detection and positioning using autodistilled models. The system detects players, court landmarks, and projects (x, y) positions onto a minimap. The annotations were generated through a fully automated process without human intervention, demonstrating the power of foundation models for dataset creation.</p>
    </div>
  </div>
</section>



  <section class="section">
    <div class="container is-max-desktop">
 
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
	    Traditional sports analytics and computer vision models rely on manually annotated datasets, which are expensive and time-consuming to create. Tennis Autodistillation introduces an automated approach to dataset generation by leveraging foundation modelsâ€”large, general-purpose AI models capable of zero-shot understanding. This project employs <strong>ChatGPT Vision (GPT-4V)</strong> and <a href="https://arxiv.org/abs/2401.14159"><strong>Grounded SAM</strong></a> to automatically classify frames, detect players, and identify court landmarks from raw tennis match footage. These auto-labeled datasets are then used to train lightweight, task-specific models that can efficiently perform player detection and court keypoint estimation in real-time. By using foundation models for initial labeling, we eliminate the need for manual annotation, enabling rapid dataset creation for model training. The <a href="https://github.com/autodistill/autodistill"><strong>Roboflow Autodistill</strong></a> library automates the process of transferring knowledge from powerful but resource-intensive models to smaller, more efficient models, making real-time tennis analysis feasible even on consumer hardware. The final trained models can take a raw video of a tennis match and generate an overhead minimap that tracks player positions, opening doors for automated sports analytics, tactical insights, and AI-powered coaching tools. This method not only applies to tennis but can be extended to other sports, surveillance, and autonomous systems where structured datasets are essential.
	    </p>
          </div>
        </div>
      </div>
  
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Project Pipeline</h2>
          <div class="content has-text-justified">
            <p>
	    The Project Pipeline described in this section refers to the inference pipeline, where a video is split into frames, and each frame is processed sequentially by the models previously trained.  First, a frame classificationa model categorizes the frame type, ensuring only relevant gameplay frames are analyzed. Then, a player detection model identifies players on the court, followed by a court landmark detection model that estimates key points on the court. Finally, a homography transformation



 		<ul>
  <li>
    <strong>Frame Extraction & Classification:</strong> The video is broken into frames, and each frame is classified into categories: gameplay (full court view), partial view (only part of court visible), close-up (player close-ups), or ignore (irrelevant frames). This ensures we focus on frames where the whole court and players are visible.
  </li>
  <li>
    <strong>Player Detection (Bounding Boxes):</strong> For each gameplay frame, we detect tennis players on the court. We use <a href="https://arxiv.org/abs/2401.14159">Grounded SAM</a> â€“ which combines an open-set object detector with the Segment Anything Model â€“ to find players based on the textual prompt "tennis player". Grounded SAM outputs segmentation masks of the players, which we convert to bounding boxes. These auto-labeled bounding boxes form a training dataset to train a smaller Tennis Player Detector (e.g. a YOLO model).
  </li>
  <li>
    <strong>Court Landmark Detection (Keypoints):</strong> We identify key court landmarks (e.g. court corner points and line intersections) in each gameplay frame. Using Grounded SAM, we first segment the tennis courtâ€™s playable area (the white-line boundary). From this mask, we estimate the four corner points of the court. Given the known geometry of a tennis court, we compute a homography (perspective transform) using the corner points and project the full set of 14 court keypoints (such as service line intersections, net posts, etc.) onto the image. This provides labeled pixel coordinates for all important landmarks, used to train a Court Keypoints Detection model (a pose estimation model that predicts these 14 points).
  </li>
  <li>
    <strong>Homography & Minimap Generation:</strong> With the trained models, each new video frame can be processed: the classifier selects gameplay frames, the player detector finds players, and the landmarks model finds court points. Finally, we compute the homography for each frame to map player positions from the video frame into an overhead court coordinate system. The output is an annotated minimap of the tennis court for each frame, showing icons or markers for player positions on a schematic of the court. This minimap is essentially a birdâ€™s-eye view tracking the players as the rally unfolds.
  </li>
</ul>

	    </p>
            <div style="max-width: 700px; margin: auto;">
              <img src="./images/representations.jpg">
            </div>
            <br>
            <br>
            <p>
              Adaptive level-of-details is crucial to scalability and quality. Sparse voxels with uniform voxel size can not scale up.
            </p>
            <div style="max-width: 400px; margin: auto;">
              <img src="./images/main_ablation.jpg">
            </div>
            <br>
            <br>
            <p>
              We sort voxels by ray direction-dependent Morton order, which ensures correct primitive blending order with mathematical proof. Sorting by primtive centers like 3DGS can produce inaccurate rendering.
              The inaccurate sorting causes 3DGS popping artifact (see left video below) while we don't have this issue (right video).
            </p>
            <div style="margin: auto;">
              <img src="./images/morton_order.jpg">
            </div>
            <video poster="" autoplay controls muted loop playsinline height="100%">
              <source src="./videos/garden_sidebyside.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Novel-view Synthesis Results</h2>
          <div class="content has-text-justified">
            <div style="max-width: 700px; margin: auto;">
              <img src="./images/quantitative_nvs.jpg">
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example1" class="bal-container-small">
            <div class="bal-after">
              <img src="./images/qualitative/bicycle_ours.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            <div class="bal-before" style="width:96.4968152866242%;">
              <div class="bal-before-inset" style="width: 512px;">
                <img src="./images/qualitative/bicycle_3dgs.png">
                <div class="bal-beforePosition beforeLabel">
                  3DGS
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example2" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/stump_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/stump_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example3" class="bal-container-small">
            <div class="bal-after">
              <img src="./images/qualitative/playroom_ours.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            <div class="bal-before" style="width:96.4968152866242%;">
              <div class="bal-before-inset" style="width: 512px;">
                <img src="./images/qualitative/playroom_3dgs.png">
                <div class="bal-beforePosition beforeLabel">
                  3DGS
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example4" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/drjohnson_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/drjohnson_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div id="example5" class="bal-container-small">
            <div class="bal-after">
              <img src="./images/qualitative/train_ours.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Ours
              </div>
            </div>
            <div class="bal-before" style="width:96.4968152866242%;">
              <div class="bal-before-inset" style="width: 512px;">
                <img src="./images/qualitative/train_3dgs.png">
                <div class="bal-beforePosition beforeLabel">
                  3DGS
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left:96.4968152866242%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content">
              <div id="example6" class="bal-container-small">
                <div class="bal-after">
                  <img src="./images/qualitative/truck_ours.png">
                  <div class="bal-afterPosition afterLabel" style="z-index:1;">
                    Ours
                  </div>
                </div>
                <div class="bal-before" style="width:62.10191082802548%;">
                  <div class="bal-before-inset" style="width: 512px;">
                    <img src="./images/qualitative/truck_3dgs.png">
                    <div class="bal-beforePosition beforeLabel">
                      3DGS
                    </div>
                  </div>
                </div>
                <div class="bal-handle" style="left:62.10191082802548%;">
                  <span class="handle-left-arrow"></span>
                  <span class="handle-right-arrow"></span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Adaptive Sparse Voxel Fusion</h2>
          <div class="content has-text-justified">
            <p>
              Fusing 2D modalities into the trained sparse voxels is efficient. The grid points simply take the weighted sum from the 2D views following classical volume fusing method. We show several examples in the following.
            </p>
            <p>
              <strong>Rendered depths &rarr; Sparse-voxel SDF &rarr; Mesh</strong>
            </p>
            <div style="margin: auto;">
              <img src="./images/meshing.jpg">
            </div>
            <br>
            <br>
            <p>
              <strong>Image segmentation by Segformer &rarr; Sparse-voxel semantic field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_segformer.ipynb">here</a>.
            </p>
            <div class="text-columns">
              <div class="text-column">Render view</div>
              <div class="text-column">3D fused semantic field</div>
              <div class="text-column">2D Segformer predction</div>
            </div>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_segformer_bicycle.mp4"
                      type="video/mp4">
            </video>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_segformer_room.mp4"
                      type="video/mp4">
            </video>
            <br>
            <br>
            <p>
              <strong>Vision foundation model feature by RADIOv2.5 &rarr; Voxel pooling &rarr; Sparse-voxel foudation feature field</strong>
              <br>
              Check it in jupyter notebook <a href="https://github.com/NVlabs/svraster/blob/main/notebooks/demo_vfm_radio.ipynb">here</a>.
            </p>
            <div class="text-columns">
              <div class="text-column">Render view</div>
              <div class="text-column">3D fused RADIO feature field</div>
              <div class="text-column">2D RADIO predction</div>
            </div>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_radio_bonsai.mp4"
                      type="video/mp4">
            </video>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_radio_garden.mp4"
                      type="video/mp4">
            </video>
            <br>
            <br>
            <p>
              <strong>Dense CLIP feature by LangSplat &rarr; Voxel pooling &rarr; Sparse-voxel language field</strong>
            </p>
            <video controls muted loop playsinline height="100%">
              <source src="./videos/fusion_langfeat.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <script>
    new BeforeAfter({
      id: '#example1'
    });
    new BeforeAfter({
      id: '#example2'
    });
    new BeforeAfter({
      id: '#example3'
    });
    new BeforeAfter({
      id: '#example4'
    });
    new BeforeAfter({
      id: '#example5'
    });
    new BeforeAfter({
      id: '#example6'
    });
  </script>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
<code>@article{Sun2024SVR,
  title={Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering},
  author={Cheng Sun and Jaesung Choe and Charles Loop and Wei-Chiu Ma and Yu-Chiang Frank Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.04459},
}</code>
      </pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!-- <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a> -->
        <!-- <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i> -->
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              We thank the authors of
              We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the
              template of this website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
